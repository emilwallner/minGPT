{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import datetime\n",
    "from mingpt.md import MemData\n",
    "from mingpt.math_dataset import MathDataset\n",
    "from mingpt.model import GPT, GPTConfig, GPT1Config\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "from mingpt.adaptive_examiner import AdaptiveExaminer\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset\n",
    "!rm -rf run models\n",
    "!cp -r data run\n",
    "!mkdir models\n",
    "fn_data = 'run/numbers__list_prime_factors.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add memory data structure to training data\n",
    "memory_slots = 7\n",
    "MD = MemData(memory_slots)\n",
    "MD.initiate_mem_slot_data(fn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_test = 'run/test_numbers__list_prime_factors.txt'\n",
    "fn_train = 'run/train_numbers__list_prime_factors.txt'\n",
    "train_dataset = MathDataset(fname=fn_train, MD=MD, marker_data=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3, 5, 17finish'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MD.tensor2string(train_dataset[4][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MD.block_size)\n",
    "print(MD.vocab_size)\n",
    "print(MD.max_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a baby GPT model\n",
    "#mconf = GPTConfig(MD.vocab_size, MD.block_size,\n",
    "#                  n_layer=4, n_head=8, n_embd=256)\n",
    "#model = GPT(mconf)\n",
    "model = torch.load('9.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_it = 100\n",
    "current_it = 0\n",
    "\n",
    "exp_folder = 'models/' + datetime.datetime.now().strftime('%Y-%m-%d~%H:%M:%S')\n",
    "examiner = AdaptiveExaminer(MD)\n",
    "\n",
    "while(current_it < max_it):\n",
    "    \n",
    "    # Wait until the working memory is filled, then use 5 epochs\n",
    "    epoch = 1 if current_it < 7 else 5\n",
    "    # Use marker data once the working memory is full\n",
    "    marker_data = 0.0 if current_it < 7 else 0.2\n",
    "    \n",
    "    # Switch between main training and marker training\n",
    "    print(\"Loading Main Dataset\\n\")\n",
    "    train_dataset = MathDataset(fname=fn_train, MD=MD, marker_data=marker_data)\n",
    "    test_dataset = MathDataset(fname=fn_test, MD=MD, marker_data=0.0)\n",
    "    \n",
    "    # Trainer Config\n",
    "    tconf = TrainerConfig(max_epochs=epoch, batch_size=358, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=1024, final_tokens=epoch*len(train_dataset)*(MD.vocab_size+1),\n",
    "                      num_workers=6)\n",
    "    \n",
    "    # Create the first training round\n",
    "    print(\"Training: \", str(current_it))\n",
    "    trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "    #trainer.train()\n",
    "    trainer.save_checkpoint(exp_folder, str(current_it))\n",
    "    \n",
    "    # Examine the model and create new dataset\n",
    "    if current_it % 2 == 0:\n",
    "        print(\"Exam and new dataset-------------\\n\")\n",
    "        print(\"Training exam \\n\")\n",
    "        examiner.exam(fn_train, trainer)\n",
    "        print(\"Test exam \\n\")\n",
    "        examiner.exam(fn_test, trainer)\n",
    "    \n",
    "    current_it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "(random.random() < 1.0) and False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
