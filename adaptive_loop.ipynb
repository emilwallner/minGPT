{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import datetime\n",
    "from mingpt.md import MemData\n",
    "from mingpt.math_dataset import MathDataset\n",
    "from mingpt.model import GPT, GPTConfig, GPT1Config\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "from mingpt.adaptive_examiner import AdaptiveExaminer\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset\n",
    "!rm -rf run models\n",
    "!cp -r data run\n",
    "#!mkdir models\n",
    "fn_data = 'run/numbers__list_prime_factors.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add memory data structure to training data\n",
    "memory_slots = 7\n",
    "MD = MemData(memory_slots, debug=0)\n",
    "MD.initiate_mem_slot_data(fn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_test = 'run/test_numbers__list_prime_factors.txt'\n",
    "fn_train = 'run/train_numbers__list_prime_factors.txt'\n",
    "train_dataset = MathDataset(fname=fn_train, MD=MD, marker_data=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MD.tensor2string(train_dataset[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MD.block_size)\n",
    "print(MD.vocab_size)\n",
    "print(MD.max_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a baby GPT model\n",
    "mconf = GPTConfig(MD.vocab_size, MD.block_size,\n",
    "                 n_layer=4, n_head=8, n_embd=256)\n",
    "model = GPT(mconf)\n",
    "#model = torch.load('12.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_it = 100\n",
    "current_it = 0\n",
    "batch_size = 384\n",
    "marker_data = 0.2\n",
    "\n",
    "exp_folder = 'models/' + datetime.datetime.now().strftime('%Y-%m-%d~%H:%M:%S')\n",
    "\n",
    "while(current_it < max_it):\n",
    "    \n",
    "    # Wait until the working memory is filled, then use 5 epochs\n",
    "    epoch = 1 if current_it < 7 else 10\n",
    "    # Use marker data once the working memory is full\n",
    "    #marker_data = 0.0 if current_it < 7 else 0.2\n",
    "    ac = 1 if current_it < 7 else 10\n",
    "    \n",
    "    examiner = AdaptiveExaminer(MD, ac=ac, max_batch=batch_size)\n",
    "    \n",
    "    # Switch between main training and marker training\n",
    "    print(\"Marker Data: \", str(marker_data))\n",
    "    train_dataset = MathDataset(fname=fn_train, MD=MD, marker_data=marker_data)\n",
    "    test_dataset = MathDataset(fname=fn_test, MD=MD, marker_data=0.0)\n",
    "    \n",
    "    # Trainer Config\n",
    "    tconf = TrainerConfig(max_epochs=epoch, batch_size=batch_size, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=1024, final_tokens=epoch*len(train_dataset)*(MD.vocab_size+1),\n",
    "                      num_workers=6)\n",
    "    \n",
    "    # Create the first training round\n",
    "    print(\"Training: \", str(current_it))\n",
    "    trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "    trainer.train()\n",
    "    trainer.save_checkpoint(exp_folder, str(current_it))\n",
    "    \n",
    "    # Examine the model and create new dataset\n",
    "    \n",
    "    print(\"Exam and new dataset-------------\\n\")\n",
    "    print(\"Training exam \\n\")\n",
    "    examiner.exam(fn_train, trainer)\n",
    "    print(\"Test exam \\n\")\n",
    "    examiner.exam(fn_test, trainer, test=True)\n",
    "    \n",
    "    current_it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
