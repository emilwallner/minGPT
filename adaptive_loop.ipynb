{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import datetime\n",
    "from mingpt.md import MemData\n",
    "from mingpt.math_dataset import MathDataset\n",
    "from mingpt.ac_schedule import AcSchedule\n",
    "from mingpt.model import GPT, GPTConfig, GPT1Config\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "from mingpt.adaptive_examiner import AdaptiveExaminer\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset\n",
    "!rm -rf run models\n",
    "!cp -r data run\n",
    "#!mkdir models\n",
    "fn_data = 'run/numbers__list_prime_factors.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add memory data structure to training data\n",
    "memory_slots = 7\n",
    "MD = MemData(memory_slots, debug=0)\n",
    "MD.initiate_mem_slot_data(fn_data, warmup_sz=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_test = 'run/test_numbers__list_prime_factors.txt'\n",
    "fn_train = 'run/train_numbers__list_prime_factors.txt'\n",
    "train_dataset = MathDataset(fname=fn_train, MD=MD, marker_data=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MD.tensor2string(train_dataset[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MD.block_size)\n",
    "print(MD.vocab_size)\n",
    "print(MD.max_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a baby GPT model\n",
    "mconf = GPTConfig(MD.vocab_size, MD.block_size,\n",
    "                 n_layer=4, n_head=8, n_embd=256)\n",
    "model = GPT(mconf)\n",
    "#model = torch.load('12.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_it = 100\n",
    "current_it = 0\n",
    "batch_size = 384\n",
    "marker_data = 0.2\n",
    "\n",
    "exp_folder = 'models/' + datetime.datetime.now().strftime('%Y-%m-%d~%H:%M:%S')\n",
    "schedule = AcSchedule(MD)\n",
    "\n",
    "while(current_it < max_it):\n",
    "    \n",
    "    epoch, size, ac, marker_data, warmup = schedule.create(current_it)\n",
    "    \n",
    "    # Switch between main training and marker training\n",
    "    print(f\"Training Iteration: {current_it}\\n\\nEpochs: {epoch}\\nSize: {size}\\nMarker: {marker_data}\\nWarm: {warmup}\\n\")\n",
    "    \n",
    "    train_dataset = MathDataset(fname=fn_train, MD=MD, marker_data=marker_data)\n",
    "    test_dataset = MathDataset(fname=fn_test, MD=MD, marker_data=0.0) if not warmup else None\n",
    "    \n",
    "    # Trainer Config\n",
    "    tconf = TrainerConfig(max_epochs=epoch, batch_size=batch_size, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=1024, final_tokens=epoch*len(train_dataset)*(MD.block_size/3),\n",
    "                      num_workers=6)\n",
    "    \n",
    "    # Create the first training round\n",
    "    trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "    trainer.train()\n",
    "    trainer.save_checkpoint(exp_folder, str(current_it))\n",
    "    \n",
    "    # Examine the model and create new dataset\n",
    "    examiner = AdaptiveExaminer(MD, ac=ac, max_batch=batch_size, warmup=warmup)\n",
    "    \n",
    "    print(\"Training exam \\n\")\n",
    "    examiner.exam(fn_train, trainer, size, test=False)\n",
    "    \n",
    "    if not warmup:\n",
    "        print(\"Test exam \\n\")\n",
    "        examiner.exam(fn_test, trainer, size, test=True)\n",
    "    \n",
    "    current_it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
