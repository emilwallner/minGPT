{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://storage.googleapis.com/dm-math-dataset/numbers__list_prime_factors.txt > numbers__list_prime_factors.txt\n",
    "!curl https://storage.googleapis.com/dm-math-dataset/numbers__is_prime.txt > numbers__is_prime.txt\n",
    "!curl https://storage.googleapis.com/dm-math-dataset/numbers__place_value.txt > numbers__place_value.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AdditionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns addition problems of up to some number of digits in the inputs. Recall\n",
    "    that all GPT cares about are sequences of integers, and completing them according to\n",
    "    patterns in the data. Therefore, we have to somehow encode addition problems\n",
    "    as a sequence of integers.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fname, split):\n",
    "        self.split = split # train/test\n",
    "        self.vocab = ['pad', 'answer', 'end'] + list(' ' + string.punctuation + string.digits + string.ascii_uppercase + string.ascii_lowercase)\n",
    "        self.vocab_size = len(self.vocab) # 10 possible digits 0..9\n",
    "        # Max input characters plus max answer characters\n",
    "        self.block_size = 160 + 32\n",
    "        self.t = {k: v for v, k in enumerate(vocab)} # Character to ID\n",
    "        self.idx = {v: k for k, v in self.t.items()} # ID to Character\n",
    "        \n",
    "        # split up all addition problems into either training data or test data\n",
    "        with open(fname, \"r\") as file:\n",
    "            text = file.read()[:-1] # Excluding the final linebreak\n",
    "            text_list = text.split('\\n')\n",
    "            self.src = text_list[0:][::2]\n",
    "            self.trg = text_list[1:][::2]\n",
    "            self.src_trg = [src+trg for src,trg in zip(self.src,self.trg)]\n",
    "        \n",
    "        self.block_size = len(max(self.src_trg, key=len)) \n",
    "        data_len = len(self.src) # total number of possible combinations\n",
    "        r = np.random.RandomState(1337) # make deterministic\n",
    "        perm = r.permutation(data_len)\n",
    "        num_test = int(data_len*0.1) # 20% of the whole dataset, or only up to 1000\n",
    "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ixes.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # given a problem index idx, first recover the associated a + b\n",
    "        idx = self.ixes[idx]\n",
    "        src = self.src[idx]\n",
    "        trg = self.trg[idx]\n",
    "        print(src, trg)\n",
    "        src_trg = list(src) + ['answer'] + list(trg) + ['end']\n",
    "        \n",
    "        src_trg = [self.t[tok] for tok in src_trg] # convert each character to its token index\n",
    "        # x will be input to GPT and y will be the associated expected outputs\n",
    "        x = [self.t['pad']] * self.block_size\n",
    "        y = [self.t['pad']] * self.block_size\n",
    "        x[:len(src_trg[:-1])] = src_trg[:-1]\n",
    "        y[:len(src_trg[1:])] = src_trg[1:] # predict the next token in the sequence\n",
    "        \n",
    "        print(x,y)\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        y = torch.tensor(y, dtype=torch.long) \n",
    "        y[:len(src)] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy = 'data/numbers__place_value.txt'\n",
    "medium = 'data/numbers__is_prime.txt'\n",
    "hard = 'data/numbers__list_prime_factors.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AdditionDataset(fname=easy, split='train')\n",
    "test_dataset = AdditionDataset(fname=easy, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the millions digit of 19847583? 9\n",
      "[68, 79, 72, 91, 3, 80, 90, 3, 91, 79, 76, 3, 84, 80, 83, 83, 80, 86, 85, 90, 3, 75, 80, 78, 80, 91, 3, 86, 77, 3, 37, 45, 44, 40, 43, 41, 44, 39, 24, 1, 45, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [79, 72, 91, 3, 80, 90, 3, 91, 79, 76, 3, 84, 80, 83, 83, 80, 86, 85, 90, 3, 75, 80, 78, 80, 91, 3, 86, 77, 3, 37, 45, 44, 40, 43, 41, 44, 39, 24, 1, 45, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([68, 79, 72, 91,  3, 80, 90,  3, 91, 79, 76,  3, 84, 80, 83, 83, 80, 86,\n",
       "         85, 90,  3, 75, 80, 78, 80, 91,  3, 86, 77,  3, 37, 45, 44, 40, 43, 41,\n",
       "         44, 39, 24,  1, 45,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100,   45,    2,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
