{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GPT on addition\n",
    "\n",
    "Train a GPT model on a dedicated addition dataset to see if a Transformer can learn to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AdditionDataset(SequentialSampler):\n",
    "    \"\"\"\n",
    "    Returns addition problems of up to some number of digits in the inputs. Recall\n",
    "    that all GPT cares about are sequences of integers, and completing them according to\n",
    "    patterns in the data. Therefore, we have to somehow encode addition problems\n",
    "    as a sequence of integers.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fname, split):\n",
    "        self.split = split # train/test\n",
    "        self.vocab = ['pad', 'answer', 'end'] + list(' ' + string.punctuation + string.digits + string.ascii_uppercase + string.ascii_lowercase)\n",
    "        self.vocab_size = len(self.vocab) # 10 possible digits 0..9\n",
    "        # Max input characters plus max answer characters\n",
    "        # self.block_size = 160 + 32\n",
    "        self.t = {k: v for v, k in enumerate(self.vocab)} # Character to ID\n",
    "        self.idx = {v: k for k, v in self.t.items()} # ID to Character\n",
    "        \n",
    "        # split up all addition problems into either training data or test data\n",
    "        with open(fname, \"r\") as file:\n",
    "            text = file.read()[:-1] # Excluding the final linebreak\n",
    "            text_list = text.split('\\n')\n",
    "            self.src = text_list[0:][::2]\n",
    "            self.trg = text_list[1:][::2]\n",
    "            self.src_trg = [src+trg for src,trg in zip(self.src,self.trg)]\n",
    "            self.max_trg = np.ceil((sum(map(len, self.trg)) / len(self.trg)))\n",
    "        \n",
    "        self.block_size = len(max(self.src_trg, key=len)) + 1\n",
    "        data_len = len(self.src) # total number of possible combinations\n",
    "        r = np.random.RandomState(1337) # make deterministic\n",
    "        perm = r.permutation(data_len)\n",
    "       \n",
    "        num_test = int(data_len*0.1) # 20% of the whole dataset, or only up to 1000\n",
    "        \n",
    "        # Sort test data by lenght to batch predictions\n",
    "        test_data_by_lenght = []\n",
    "        for index in perm[num_test:]:\n",
    "            test_data_by_lenght.append([index, len(self.src[index])])\n",
    "        test_data_by_lenght = sorted(test_data_by_lenght, key=lambda x: x[1])\n",
    "        test_data_by_lenght = [i[0] for i in test_data_by_lenght]\n",
    "        \n",
    "        self.ixes = np.array(test_data_by_lenght) if split == 'test' else perm[num_test:]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ixes.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # given a problem index idx, first recover the associated a + b\n",
    "        idx = self.ixes[idx]\n",
    "        src = self.src[idx]\n",
    "        trg = self.trg[idx]\n",
    "\n",
    "        src_trg = list(src) + ['answer'] + list(trg) + ['end']\n",
    "        src_trg = [self.t[tok] for tok in src_trg] # convert each character to its token index\n",
    "        \n",
    "        # x will be input to GPT and y will be the associated expected outputs\n",
    "        x = [self.t['pad']] * self.block_size\n",
    "        y = [self.t['pad']] * self.block_size\n",
    "  \n",
    "        x[:len(src_trg[:-1])] = src_trg[:-1]\n",
    "        y[:len(src_trg[1:])] = src_trg[1:] # predict the next token in the sequence\n",
    "        y = [-100 if tok == self.t['pad'] else tok for tok in y] # -100 will mask loss to zero\n",
    "\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        y = torch.tensor(y, dtype=torch.long) \n",
    "        y[:len(src)] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
    "        \n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "265367\n",
      "List the prime factors of 737.\n",
      "List the prime factors of 8812.\n",
      "List the prime factors of 6140.\n",
      "List the prime factors of 2981.\n",
      "List the prime factors of 6375.\n",
      "List the prime factors of 5746.\n",
      "List the prime factors of 2611.\n",
      "List the prime factors of 5050.\n",
      "List the prime factors of 8255.\n",
      "List the prime factors of 9419.\n",
      "List the prime factors of 3095.\n",
      "List the prime factors of 5648.\n",
      "List the prime factors of 91530.\n",
      "List the prime factors of 10555.\n",
      "List the prime factors of 52123.\n",
      "List the prime factors of 70014.\n",
      "List the prime factors of 96701.\n",
      "List the prime factors of 21729.\n",
      "List the prime factors of 52256.\n",
      "List the prime factors of 82715.\n",
      "List the prime factors of 67608.\n",
      "List the prime factors of 54251.\n",
      "List the prime factors of 23251.\n",
      "List the prime factors of 75291.\n",
      "List the prime factors of 54774.\n",
      "List the prime factors of 82729.\n",
      "List the prime factors of 94351.\n",
      "List the prime factors of 67747.\n",
      "List the prime factors of 13489.\n",
      "List the prime factors of 32433.\n",
      "List the prime factors of 79377.\n",
      "List the prime factors of 25526.\n",
      "List the prime factors of 88105.\n",
      "List the prime factors of 96021.\n",
      "List the prime factors of 62898.\n",
      "List the prime factors of 74030.\n",
      "List the prime factors of 98433.\n",
      "List the prime factors of 85255.\n",
      "List the prime factors of 57754.\n",
      "List the prime factors of 51375.\n",
      "List the prime factors of 79765.\n",
      "List the prime factors of 99645.\n",
      "List the prime factors of 18708.\n",
      "List the prime factors of 17688.\n",
      "List the prime factors of 48099.\n",
      "List the prime factors of 36208.\n",
      "List the prime factors of 61173.\n",
      "List the prime factors of 13903.\n",
      "List the prime factors of 56595.\n",
      "List the prime factors of 16356.\n",
      "List the prime factors of 94134.\n",
      "List the prime factors of 19375.\n",
      "List the prime factors of 53511.\n",
      "List the prime factors of 99917.\n",
      "List the prime factors of 58536.\n",
      "List the prime factors of 47632.\n",
      "List the prime factors of 63841.\n",
      "List the prime factors of 42663.\n",
      "List the prime factors of 33181.\n",
      "List the prime factors of 90662.\n",
      "List the prime factors of 58262.\n",
      "List the prime factors of 67892.\n",
      "List the prime factors of 76729.\n",
      "List the prime factors of 83077.\n",
      "List the prime factors of 12306.\n",
      "List the prime factors of 70254.\n",
      "List the prime factors of 81544.\n",
      "List the prime factors of 24427.\n",
      "List the prime factors of 58476.\n",
      "List the prime factors of 97875.\n",
      "List the prime factors of 88382.\n",
      "List the prime factors of 29780.\n",
      "List the prime factors of 79616.\n",
      "List the prime factors of 12460.\n",
      "List the prime factors of 51104.\n",
      "List the prime factors of 45828.\n",
      "List the prime factors of 97740.\n",
      "List the prime factors of 77447.\n",
      "List the prime factors of 84832.\n",
      "List the prime factors of 95599.\n",
      "List the prime factors of 11443.\n",
      "List the prime factors of 87681.\n",
      "List the prime factors of 56460.\n",
      "List the prime factors of 75061.\n",
      "List the prime factors of 81851.\n",
      "List the prime factors of 98937.\n",
      "List the prime factors of 46689.\n",
      "List the prime factors of 52151.\n",
      "List the prime factors of 44242.\n",
      "List the prime factors of 40757.\n",
      "List the prime factors of 11680.\n",
      "List the prime factors of 76500.\n",
      "List the prime factors of 96947.\n",
      "List the prime factors of 94866.\n",
      "List the prime factors of 98561.\n",
      "List the prime factors of 35843.\n",
      "List the prime factors of 40841.\n",
      "List the prime factors of 26533.\n",
      "List the prime factors of 16532.\n",
      "List the prime factors of 88281.\n"
     ]
    }
   ],
   "source": [
    "# create a dataset \n",
    "easy = 'data/numbers__place_value.txt'\n",
    "medium = 'data/numbers__is_prime.txt'\n",
    "hard = 'data/numbers__list_prime_factors.txt'\n",
    "\n",
    "#train_dataset = AdditionDataset(fname=hard, split='train')\n",
    "test_dataset = AdditionDataset(fname=hard, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e14f2579798a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# for i in range(0, 10):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#     print(test_dataset[i][0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "# for i in range(0, len(train_dataset)):\n",
    "#     if len(train_dataset[i][0]) != 52 or len(train_dataset[i][1]) != 52:\n",
    "#         print(train_dataset.block_size)\n",
    "#         print(len(train_dataset[i][0]))\n",
    "#         print(len(train_dataset[i][1]))\n",
    "#         print(train_dataset[i])\n",
    "\n",
    "# loader = DataLoader(test_dataset, shuffle=False)\n",
    "# i = 0\n",
    "# for b, (x, y) in enumerate(loader):\n",
    "#     test = x.tolist()\n",
    "#     x_str = ''.join([test_dataset.idx[tok] for tok in test[0]])\n",
    "#     print(x_str)\n",
    "#     i += 1\n",
    "#     if i == 10:\n",
    "#         break\n",
    "\n",
    "print(''.join([test_dataset.idx[tok] for tok in test_dataset[0].tolist()[0]]))\n",
    "# for i in range(0, 10):\n",
    "#     print(test_dataset[i][0])\n",
    "    #print(count)\n",
    "    #print(len(count)) # sample a training instance just to see what one raw example looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.model import GPT, GPTConfig, GPT1Config\n",
    "\n",
    "# initialize a baby GPT model\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, \n",
    "                  n_layer=2, n_head=4, n_embd=128)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=1, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=1024, final_tokens=50*len(train_dataset)*(14+1),\n",
    "                      num_workers=0)\n",
    "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's give the trained model an addition exam\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from mingpt.utils import sample\n",
    "\n",
    "def give_exam(dataset, batch_size=32, max_batches=-1):\n",
    "    \n",
    "    results = []\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    for b, (x, y) in enumerate(loader):\n",
    "        \n",
    "        cut = x[0].tolist().index(dataset.t['answer']) + 1\n",
    "        pad = -2 if dataset.t['pad'] not in x[0].tolist() else x[0].tolist().index(dataset.t['pad'])\n",
    "        x_in = x[:, :cut]\n",
    "        print(x_in)\n",
    "        pred = sample(model, x_in, int(dataset.max_trg+1))\n",
    "\n",
    "        for i in range(x.size(0)):\n",
    "\n",
    "            #x_list = x[:, cut+1:pad].tolist()[0]\n",
    "            #y_list = pred[:, cut+1:pad].tolist()[0]\n",
    "            x_list = x[:, cut:pad].tolist()[0]\n",
    "            y_list = pred[:, cut:pad].tolist()[0]\n",
    "            x_str = ''.join([dataset.idx[tok] for tok in x_list])\n",
    "            y_str = ''.join([dataset.idx[tok] for tok in y_list])\n",
    "            \n",
    "            correct = 1 if x_str == y_str else 0\n",
    "     \n",
    "            results.append(correct)\n",
    "            judge = 'YEP!!!' if correct else 'NOPE'\n",
    "            #if not correct:\n",
    "\n",
    "            question =  x[:, :cut-1].tolist()[0]\n",
    "            question_str = ''.join([dataset.idx[tok] for tok in question])\n",
    "                \n",
    "            print(\"Q: %s\\nP:%s\\nG:%s\" % (question_str, y_str, x_str))\n",
    "        \n",
    "        if max_batches >= 0 and b+1 >= max_batches:\n",
    "            break\n",
    "\n",
    "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training set: how well did we memorize?\n",
    "give_exam(test_dataset, batch_size=1, max_batches=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set: how well did we generalize?\n",
    "give_exam(train_dataset, batch_size=1024, max_batches=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that's amusing... our model learned everything except 55 + 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ['-1', '-1', '2', '1', '1']\n",
    "\n",
    "it.takewhile(lambda x: x!='2', f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.RandomState(1338)\n",
    "perm = r.permutation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
